{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14241,"databundleVersionId":862020,"sourceType":"competition"},{"sourceId":10403430,"sourceType":"datasetVersion","datasetId":6446530},{"sourceId":10403652,"sourceType":"datasetVersion","datasetId":6446705},{"sourceId":10412038,"sourceType":"datasetVersion","datasetId":6452742},{"sourceId":222715,"sourceType":"modelInstanceVersion","modelInstanceId":189997,"modelId":211990},{"sourceId":223835,"sourceType":"modelInstanceVersion","modelInstanceId":190964,"modelId":212916},{"sourceId":226990,"sourceType":"modelInstanceVersion","modelInstanceId":193560,"modelId":215488}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! ls ../input/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:06:11.992141Z","iopub.execute_input":"2025-08-07T20:06:11.992427Z","iopub.status.idle":"2025-08-07T20:06:12.110387Z","shell.execute_reply.started":"2025-08-07T20:06:11.992405Z","shell.execute_reply":"2025-08-07T20:06:12.109342Z"}},"outputs":[{"name":"stdout","text":"model_new\tnew-sample  severstal-steel-defect-detection  test-image\nmodel_sgformer\tours_unet   submission-basic\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pdb\nimport os\nimport cv2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.pytorch import ToTensorV2\nimport torch.utils.data as data\n# import segmentation_models_pytorch as smp","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:08:00.358509Z","iopub.execute_input":"2025-08-07T20:08:00.358785Z","iopub.status.idle":"2025-08-07T20:08:00.363327Z","shell.execute_reply.started":"2025-08-07T20:08:00.358766Z","shell.execute_reply":"2025-08-07T20:08:00.362333Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:08:05.510611Z","iopub.execute_input":"2025-08-07T20:08:05.510939Z","iopub.status.idle":"2025-08-07T20:08:05.515716Z","shell.execute_reply.started":"2025-08-07T20:08:05.510912Z","shell.execute_reply":"2025-08-07T20:08:05.514866Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class TestDataset(Dataset):\n    '''Dataset for test prediction'''\n    def __init__(self, root, df, mean, std):\n        self.root = root\n        # df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n        self.fnames = df['ImageId'].unique().tolist()\n        self.num_samples = len(self.fnames)\n        self.transform = Compose(\n            [\n                Normalize(mean=mean, std=std, p=1),\n                ToTensorV2(),\n            ]\n        )\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        path = os.path.join(self.root, fname)\n        image = cv2.imread(path)\n        images = self.transform(image=image)[\"image\"]\n        return fname, images\n\n    def __len__(self):\n        return self.num_samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:08:11.022877Z","iopub.execute_input":"2025-08-07T20:08:11.023195Z","iopub.status.idle":"2025-08-07T20:08:11.028415Z","shell.execute_reply.started":"2025-08-07T20:08:11.023171Z","shell.execute_reply":"2025-08-07T20:08:11.027396Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def post_process(probability, threshold, min_size):\n    '''Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored'''\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:08:15.112075Z","iopub.execute_input":"2025-08-07T20:08:15.112356Z","iopub.status.idle":"2025-08-07T20:08:15.117138Z","shell.execute_reply.started":"2025-08-07T20:08:15.112336Z","shell.execute_reply":"2025-08-07T20:08:15.116151Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"<!-- ImageId_ClassId -->","metadata":{}},{"cell_type":"code","source":"sample_submission_path = '/kaggle/input/new-sample/sample_submission.csv'\ntest_data_folder = \"/kaggle/input/test-image/test_images/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:08:25.055522Z","iopub.execute_input":"2025-08-07T20:08:25.055939Z","iopub.status.idle":"2025-08-07T20:08:25.060412Z","shell.execute_reply.started":"2025-08-07T20:08:25.055908Z","shell.execute_reply":"2025-08-07T20:08:25.059358Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# initialize test dataloader\nbest_threshold = 0.5\nnum_workers = 2\nbatch_size = 1\nprint('best_threshold', best_threshold)\nmin_size = 3500\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\ndf = pd.read_csv(sample_submission_path)\ntestset = DataLoader(\n    TestDataset(test_data_folder, df, mean, std),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=num_workers,\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:08:46.092970Z","iopub.execute_input":"2025-08-07T20:08:46.093305Z","iopub.status.idle":"2025-08-07T20:08:46.124557Z","shell.execute_reply.started":"2025-08-07T20:08:46.093279Z","shell.execute_reply":"2025-08-07T20:08:46.123641Z"}},"outputs":[{"name":"stdout","text":"best_threshold 0.5\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from math import sqrt\nfrom functools import partial\nimport torch\nfrom torch import nn, einsum\nimport torch.nn.functional as F\n\nfrom einops import rearrange, reduce\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef exists(val):\n    return val is not None\n\ndef cast_tuple(val, depth):\n    return val if isinstance(val, tuple) else (val,) * depth\n\n# classes\n\nclass DsConv2d(nn.Module):\n    def __init__(self, dim_in, dim_out, kernel_size, padding, stride = 1, bias = True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(dim_in, dim_in, kernel_size = kernel_size, padding = padding, groups = dim_in, stride = stride, bias = bias),\n            nn.Conv2d(dim_in, dim_out, kernel_size = 1, bias = bias)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, eps = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n\n    def forward(self, x):\n        std = torch.var(x, dim = 1, unbiased = False, keepdim = True).sqrt()\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        return (x - mean) / (std + self.eps) * self.g + self.b\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.fn = fn\n        self.norm = LayerNorm(dim)\n\n    def forward(self, x):\n        return self.fn(self.norm(x))\n\nclass EfficientSelfAttention(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        heads,\n        reduction_ratio\n    ):\n        super().__init__()\n        self.scale = (dim // heads) ** -0.5\n        self.heads = heads\n\n        self.to_q = nn.Conv2d(dim, dim, 1, bias = False)\n        self.to_kv = nn.Conv2d(dim, dim * 2, reduction_ratio, stride = reduction_ratio, bias = False)\n        self.to_out = nn.Conv2d(dim, dim, 1, bias = False)\n\n    def forward(self, x):\n        h, w = x.shape[-2:]\n        heads = self.heads\n\n        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))\n        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h = heads), (q, k, v))\n\n        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n        attn = sim.softmax(dim = -1)\n\n        out = einsum('b i j, b j d -> b i d', attn, v)\n        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h = heads, x = h, y = w)\n        return self.to_out(out)\n\nclass MixFeedForward(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        expansion_factor\n    ):\n        super().__init__()\n        hidden_dim = dim * expansion_factor\n        self.net = nn.Sequential(\n            nn.Conv2d(dim, hidden_dim, 1),\n            DsConv2d(hidden_dim, hidden_dim, 3, padding = 1),\n            nn.GELU(),\n            nn.Conv2d(hidden_dim, dim, 1)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass MiT(nn.Module):\n    def __init__(\n        self,\n        *,\n        channels,\n        dims,\n        heads,\n        ff_expansion,\n        reduction_ratio,\n        num_layers\n    ):\n        super().__init__()\n        stage_kernel_stride_pad = ((7, 4, 3), (3, 2, 1), (3, 2, 1), (3, 2, 1))\n\n        dims = (channels, *dims)\n        dim_pairs = list(zip(dims[:-1], dims[1:]))\n\n        self.stages = nn.ModuleList([])\n\n        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n            \n            get_overlap_patches = nn.Unfold(kernel, stride = stride, padding = padding)\n            \n            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n\n            layers = nn.ModuleList([])\n\n            for _ in range(num_layers):\n                layers.append(nn.ModuleList([\n                    PreNorm(dim_out, EfficientSelfAttention(dim = dim_out, heads = heads, reduction_ratio = reduction_ratio)),\n                    PreNorm(dim_out, MixFeedForward(dim = dim_out, expansion_factor = ff_expansion)),\n                ]))\n\n            self.stages.append(nn.ModuleList([\n                get_overlap_patches,\n                overlap_patch_embed,\n                layers\n            ]))\n\n    def forward(\n        self,\n        x,\n        return_layer_outputs = False\n    ):\n        h, w = x.shape[-2:]\n#         print(f'the input mix x is {x.shape}')\n        layer_outputs = []\n        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n            \n            x = get_overlap_patches(x)\n#             print(f'the x after get_overlap_patches is {x.shape}')\n            \n            num_patches = x.shape[-1]\n            ratio = int(sqrt((h * w) / num_patches))\n            x = rearrange(x, 'b c (h w) -> b c h w', h = h // ratio)\n\n            x = overlap_embed(x)\n#             print(f'the x after overlap_embed is {x.shape}')\n            \n            for (attn, ff) in layers:\n                x = attn(x) + x\n                x = ff(x) + x\n#             print(f'the mit x is {x.shape}')\n            layer_outputs.append(x)\n\n        ret = x if not return_layer_outputs else layer_outputs\n        return ret\n\nclass Segformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dims = (32, 64, 160, 256),\n        heads = (1, 2, 5, 8),\n        ff_expansion = (8, 8, 4, 4),\n        reduction_ratio = (8, 4, 2, 1),\n        num_layers = 2,\n        channels = 3,\n        decoder_dim = 256,\n        num_classes = 4\n    ):\n        super().__init__()\n        dims, heads, ff_expansion, reduction_ratio, num_layers = map(partial(cast_tuple, depth = 4), (dims, heads, ff_expansion, reduction_ratio, num_layers))\n        assert all([*map(lambda t: len(t) == 4, (dims, heads, ff_expansion, reduction_ratio, num_layers))]), 'only four stages are allowed, all keyword arguments must be either a single value or a tuple of 4 values'\n\n        self.mit = MiT(\n            channels = channels,\n            dims = dims,\n            heads = heads,\n            ff_expansion = ff_expansion,\n            reduction_ratio = reduction_ratio,\n            num_layers = num_layers\n        )\n\n        self.to_fused = nn.ModuleList([nn.Sequential(\n            nn.Conv2d(dim, decoder_dim, 1),\n            nn.Upsample(scale_factor = 2 ** (i+2)  )\n        ) for i, dim in enumerate(dims)])\n\n        self.to_segmentation = nn.Sequential(\n            nn.Conv2d(4 * decoder_dim, decoder_dim, 1),\n            nn.Conv2d(decoder_dim, num_classes, 1),\n        )\n\n    def forward(self, x):\n        layer_outputs = self.mit(x, return_layer_outputs = True)\n#         for k in range(len(layer_outputs)):\n#             print(f'the k shape is {layer_outputs[k].shape}')\n                       \n        fused = [to_fused(output) for output, to_fused in zip(layer_outputs, self.to_fused)]\n        \n#         for k in range(len(fused)):\n#             print(f'the k shape fused is {fused[k].shape}')\n        \n        fused = torch.cat(fused, dim = 1)\n        \n        \n        return self.to_segmentation(fused)\n\nmodel = Segformer()\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:09:18.775308Z","iopub.execute_input":"2025-08-07T20:09:18.775610Z","iopub.status.idle":"2025-08-07T20:09:20.096824Z","shell.execute_reply.started":"2025-08-07T20:09:18.775587Z","shell.execute_reply":"2025-08-07T20:09:20.095970Z"}},"outputs":[{"name":"stdout","text":"Segformer(\n  (mit): MiT(\n    (stages): ModuleList(\n      (0): ModuleList(\n        (0): Unfold(kernel_size=7, dilation=1, padding=3, stride=4)\n        (1): Conv2d(147, 32, kernel_size=(1, 1), stride=(1, 1))\n        (2): ModuleList(\n          (0-1): 2 x ModuleList(\n            (0): PreNorm(\n              (fn): EfficientSelfAttention(\n                (to_q): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (to_kv): Conv2d(32, 64, kernel_size=(8, 8), stride=(8, 8), bias=False)\n                (to_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              )\n              (norm): LayerNorm()\n            )\n            (1): PreNorm(\n              (fn): MixFeedForward(\n                (net): Sequential(\n                  (0): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n                  (1): DsConv2d(\n                    (net): Sequential(\n                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n                      (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n                    )\n                  )\n                  (2): GELU(approximate='none')\n                  (3): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n                )\n              )\n              (norm): LayerNorm()\n            )\n          )\n        )\n      )\n      (1): ModuleList(\n        (0): Unfold(kernel_size=3, dilation=1, padding=1, stride=2)\n        (1): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1))\n        (2): ModuleList(\n          (0-1): 2 x ModuleList(\n            (0): PreNorm(\n              (fn): EfficientSelfAttention(\n                (to_q): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (to_kv): Conv2d(64, 128, kernel_size=(4, 4), stride=(4, 4), bias=False)\n                (to_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              )\n              (norm): LayerNorm()\n            )\n            (1): PreNorm(\n              (fn): MixFeedForward(\n                (net): Sequential(\n                  (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))\n                  (1): DsConv2d(\n                    (net): Sequential(\n                      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n                      (1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n                    )\n                  )\n                  (2): GELU(approximate='none')\n                  (3): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n                )\n              )\n              (norm): LayerNorm()\n            )\n          )\n        )\n      )\n      (2): ModuleList(\n        (0): Unfold(kernel_size=3, dilation=1, padding=1, stride=2)\n        (1): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n        (2): ModuleList(\n          (0-1): 2 x ModuleList(\n            (0): PreNorm(\n              (fn): EfficientSelfAttention(\n                (to_q): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (to_kv): Conv2d(160, 320, kernel_size=(2, 2), stride=(2, 2), bias=False)\n                (to_out): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              )\n              (norm): LayerNorm()\n            )\n            (1): PreNorm(\n              (fn): MixFeedForward(\n                (net): Sequential(\n                  (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))\n                  (1): DsConv2d(\n                    (net): Sequential(\n                      (0): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n                      (1): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n                    )\n                  )\n                  (2): GELU(approximate='none')\n                  (3): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n                )\n              )\n              (norm): LayerNorm()\n            )\n          )\n        )\n      )\n      (3): ModuleList(\n        (0): Unfold(kernel_size=3, dilation=1, padding=1, stride=2)\n        (1): Conv2d(1440, 256, kernel_size=(1, 1), stride=(1, 1))\n        (2): ModuleList(\n          (0-1): 2 x ModuleList(\n            (0): PreNorm(\n              (fn): EfficientSelfAttention(\n                (to_q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (to_kv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n                (to_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n              )\n              (norm): LayerNorm()\n            )\n            (1): PreNorm(\n              (fn): MixFeedForward(\n                (net): Sequential(\n                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n                  (1): DsConv2d(\n                    (net): Sequential(\n                      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n                      (1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n                    )\n                  )\n                  (2): GELU(approximate='none')\n                  (3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n                )\n              )\n              (norm): LayerNorm()\n            )\n          )\n        )\n      )\n    )\n  )\n  (to_fused): ModuleList(\n    (0): Sequential(\n      (0): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): Upsample(scale_factor=4.0, mode='nearest')\n    )\n    (1): Sequential(\n      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): Upsample(scale_factor=8.0, mode='nearest')\n    )\n    (2): Sequential(\n      (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): Upsample(scale_factor=16.0, mode='nearest')\n    )\n    (3): Sequential(\n      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n      (1): Upsample(scale_factor=32.0, mode='nearest')\n    )\n  )\n  (to_segmentation): Sequential(\n    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n    (1): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n  )\n)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# import segmentation_models_pytorch as smp\n# Initialize mode and load trained weights\n#ckpt_path = \"../input/unetstartermodelfile/model.pth\"\nckpt_path = \"/kaggle/input/model_sgformer/pytorch/default/1/model_segformer_new.pth\"\ndevice = torch.device(\"cuda\")\n#model = Unet(\"resnet18\", encoder_weights=None, classes=4, activation=None)\n# model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)\n# model.to(device)\nmodel.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nmodel.load_state_dict(state[\"state_dict\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:09:43.457393Z","iopub.execute_input":"2025-08-07T20:09:43.457728Z","iopub.status.idle":"2025-08-07T20:09:43.588332Z","shell.execute_reply.started":"2025-08-07T20:09:43.457702Z","shell.execute_reply":"2025-08-07T20:09:43.587282Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-15-44ce7f062341>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# start prediction\npredictions = []\nfor i, batch in enumerate(tqdm(testset)):\n    fnames, images = batch\n    batch_preds = torch.sigmoid(model(images.to(device)))\n    batch_preds = batch_preds.detach().cpu().numpy()\n    for fname, preds in zip(fnames, batch_preds):\n        # print(f'the shape of pred is {preds.shape}')\n        for cls, pred in enumerate(preds):\n            # print(f'cls is {cls}')\n            pred, num = post_process(pred, best_threshold, min_size)\n            rle = mask2rle(pred)\n            name = fname + f\"_{cls+1}\"\n            # predictions.append([fname, rle, cls])\n            predictions.append([name, rle])\n            \n\n# save predictions to submission.csv\ndf = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])\n\n\n\n\n# print(f'the shape of prediction is {predictions.shape}')\n# df = df[~df.applymap(lambda x: x == '').any(axis=1)]\n# df = df[~df.applymap(lambda x: x == '').any(axis=1)]\n\n# df = pd.DataFrame(predictions,columns=['ImageId','EncodedPixels','ClassId'])\ndf = df.fillna('')\n# df = df[~df.applymap(lambda x: x == None).any(axis=1)]\n# df = df[~df.applymap(lambda x: x == '').any(axis=1)]\ndf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:09:48.209836Z","iopub.execute_input":"2025-08-07T20:09:48.210153Z","iopub.status.idle":"2025-08-07T20:20:22.859863Z","shell.execute_reply.started":"2025-08-07T20:09:48.210132Z","shell.execute_reply":"2025-08-07T20:20:22.858934Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 5506/5506 [10:34<00:00,  8.68it/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:24:22.293261Z","iopub.execute_input":"2025-08-07T20:24:22.293585Z","iopub.status.idle":"2025-08-07T20:24:22.306406Z","shell.execute_reply.started":"2025-08-07T20:24:22.293560Z","shell.execute_reply":"2025-08-07T20:24:22.305608Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   ImageId_ClassId                                      EncodedPixels\n0  0000f269f.jpg_1                                                   \n1  0000f269f.jpg_2                                                   \n2  0000f269f.jpg_3  145553 72 145629 4 145641 16 145809 72 145885 ...\n3  0000f269f.jpg_4                                                   \n4  000ccc2ac.jpg_1                                                   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId_ClassId</th>\n      <th>EncodedPixels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000f269f.jpg_1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000f269f.jpg_2</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000f269f.jpg_3</td>\n      <td>145553 72 145629 4 145641 16 145809 72 145885 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0000f269f.jpg_4</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000ccc2ac.jpg_1</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print(df.shape[0])\nnum_unique_images = df['ImageId_ClassId'].nunique()\nprint(num_unique_images)\n\n #df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n #num_unique_images = df['ImageId'].nunique()\n #print(num_unique_images)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T20:25:02.802452Z","iopub.execute_input":"2025-08-07T20:25:02.802811Z","iopub.status.idle":"2025-08-07T20:25:02.813138Z","shell.execute_reply.started":"2025-08-07T20:25:02.802782Z","shell.execute_reply":"2025-08-07T20:25:02.812259Z"}},"outputs":[{"name":"stdout","text":"22024\n22024\n","output_type":"stream"}],"execution_count":19}]}